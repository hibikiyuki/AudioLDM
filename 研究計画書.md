# 🤖研究計画書：主観的評価に基づく対話型進化的効果音生成

## 1. 研究の背景と目的

従来のテキストベースの生成モデル（Text-to-Audio）では、ユーザーが自身の脳内にある抽象的な「理想の音」を言語化し、プロンプトとして正確に入力することに限界がある。特に、オノマトペや質感、感情的なニュアンスを含む効果音（SFX）の場合、言語による記述と実際の出力の乖離が大きい。

本研究は、生成モデル（AudioLDM）と対話型進化計算（IEC: Interactive Evolutionary Computation）を統合し、ユーザーが生成された候補の中から「好みのもの」を選択・淘汰・交配させることで、言語化不可能な選好を反映した効果音生成手法を確立することを目的とする。

## 2. 研究アプローチとアルゴリズム

### 2.1 システム概要

本システムは、拡散モデルの潜在空間（Latent Space）を進化計算の探索空間とみなす「Latent Variable Evolution」を採用する。

1. **初期個体生成**: 任意のプロンプト（例：「爆発音」）またはランダムシードに基づき、AudioLDMを用いて第1世代の音声（$N$個）を生成する。
2. **主観的評価**: ユーザーは提示された音声を聴取し、好ましい個体（親個体）を選択する。
3. **進化操作（次世代生成）**:
    * **交叉（Crossover）**: 選択された親個体の潜在ベクトル（Latent Vectors）間で補間を行う。高次元空間での補間であるため、単純な線形補間ではなく球面線形補間（Slerp: Spherical Linear Interpolation）を用いるのが適切である。
    * **突然変異（Mutation）**: 潜在ベクトルに対してガウシアンノイズを付加し、探索の多様性を維持する。
4. **終了条件**: ユーザーが満足する音声が得られるまで、2-3を繰り返す。

### 2.2 映像リファレンスの導入（オプション要素の扱い）

映像（動画）に対する音響づけを行う場合、評価基準は「音単体の好み」から「映像との同期・適合性」に変化する。

* **実装方針**: 映像の特徴量（CLIP embedding等）を条件付け（Conditioning）としてAudioLDMに入力し、初期個体の探索範囲を絞り込むことが論理的である。しかし、最終的な適合性判断はユーザーの主観（IEC）に委ねる。

## 3. 実装が必要な内容リスト

### 3.1 バックエンド・コアロジック (Python/PyTorch)

* [x] **推論エンジンの構築**:
    * [x] `diffusers` ライブラリを用いたAudioLDMパイプラインのセットアップ。
    * [x] GPUメモリ管理（VRAM最適化）。対話型システムでは応答速度が重要であり、FP16精度やFlash Attentionの適用が必須。
* [ ] **進化計算アルゴリズムの実装**:
    * [ ] **Genotype管理**: 音声波形そのものではなく、拡散モデルの「ノイズ」および「条件付けベクトル」を遺伝子として保持するクラス設計。
    * [ ] **Crossover関数**: 2つの潜在テンソル間のSlerp実装。
    * [ ] **Mutation関数**: 指定されたレートでのノイズ注入ロジック。
* [ ] **音声保存・管理**: 生成された音声を一時保存し、ID管理するデータベース（簡易なJSON/SQLiteで可）。

### 3.2 フロントエンド・UI (Gradio/Streamlit/React)

* [ ] **マルチモーダル提示**:
    * [ ] 複数（例：4～8個）の音声を並列に配置し、再生ボタンと選択（Vote）ボタンを設置。
    * [ ] （オプション）映像プレビュー画面と音声再生の同期機能。
* [x] **インタラクション制御**:
    * [x] 「次世代生成（Evolve）」ボタン。
    * [x] 「突然変異率（Mutation Rate）」のスライダー（ユーザーが変化の激しさを調整可能にするため）。
* [x] **履歴管理**:
    * [x] 過去の世代に戻れる機能（進化の袋小路に入った場合の救済措置）。

### 3.3 評価実験環境

* [x] **ログ収集機能**: ユーザーの選択履歴、収束までの世代数、選好時間の計測。
* [ ] **比較用ベースライン**: ランダム生成（進化なし）との比較用インターフェース。

---

## ✅ 実装完了報告 (2026年1月8日)

### 実装されたモジュール

1. **`audioldm/iec.py`** (403行)
   - AudioGenotype: 遺伝子型クラス
   - slerp: 球面線形補間
   - crossover_slerp: 交叉関数
   - mutate_gaussian: 突然変異関数
   - IECPopulation: 個体群管理
   - adaptive_mutation_rate: 適応的変異率

2. **`audioldm/iec_pipeline.py`** (305行)
   - AudioLDM_IEC: AudioLDM統合クラス
   - 初期個体群生成
   - 進化的生成
   - 音声保存・管理

3. **`audioldm/iec_gradio.py`** (542行)
   - IECInterface: UI状態管理
   - create_gradio_interface: Gradio UI構築
   - launch_interface: サーバー起動

4. **スクリプト**
   - `scripts/launch_iec_gradio.py`: Gradio UI起動
   - `scripts/run_iec_cli.py`: CLI版実行
   - `scripts/test_iec_core.py`: テストスクリプト

### テスト結果

全てのコア機能テストが成功:
```bash
$ python scripts/test_iec_core.py
✅ 全てのテストが成功しました！
```

### 使用方法

```bash
# Gradio UI起動
python scripts/launch_iec_gradio.py

# CLI版
python scripts/run_iec_cli.py --prompt "爆発音"
```

詳細は `IEC_README.md` および `IMPLEMENTATION_REPORT.md` を参照。

---

## 4. 客観的分析と懸念されるリスク・課題

本提案手法におけるメリットと、技術的・体験的なリスク（デメリット）を客観的に分析する。

### 4.1 メリット（肯定的な側面）

* **言語化の壁の突破**: 擬音語や抽象的なイメージなど、プロンプトで表現しきれない微細な音響特性を、聴覚的なフィードバックループを通じて探索できる。
* **セレンディピティ（偶発的発見）**: ユーザーが想定していなかったが、コンテキストに合致する「意外な音」を発見できる可能性がある。これは決定論的なエンジニアリングでは得にくい。

### 4.2 リスクと課題（否定的な側面・批判的見解）

* **ユーザー疲労（User Fatigue）の深刻化**:
    * **時間的制約**: 画像（視覚）によるIECと異なり、音声（聴覚）は**時間軸を持つメディア**である。5秒の音声を8個確認するには最低40秒かかる。世代ごとの評価コストが極めて高く、数世代でユーザーが疲弊し、適当な選択を行うリスクが高い。
    * **聴覚順応**: 同じような音を連続して聴取することで、ユーザーの聴覚的な判別能力が低下（ゲシュタルト崩壊）する可能性がある。
* **推論レイテンシの問題**:
    * AudioLDMは拡散モデルであり、推論に計算コストがかかる。ユーザーが選択してから次の世代が提示されるまでに数十秒の待ち時間が発生すれば、対話型システムとして成立しない。リアルタイム性確保には、ハイエンドGPUあるいはLatent Consistency Model (LCM) などの高速化技術の導入が不可欠である。
* **局所解への収束**:
    * 初期世代の多様性が低い、あるいは選択の偏りにより、似たような音ばかりが生成される「早すぎる収束」が発生しやすい。突然変異率の動的な制御が必要となる。

## 5. 推奨される次のアクション

まず**プロトタイピングによる「評価コスト」の検証**を行うべきだ。

1. AudioLDMを用いて事前に固定のシード値で音声を生成しておく。
2. GUIを作成し、実際に人間が「聴取→選択」のループを何回（何分）耐えられるかを測定する。

このデータがないまま本実装に進むと、システムは完成したが「使うのが面倒で誰も使わない」ツールになるリスクがある。